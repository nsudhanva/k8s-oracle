---
title: Gemma 3 LLM - Self-Hosted AI on OKE
description: Deploy Gemma 3 1B language model on your OKE cluster using Ollama. OpenAI-compatible API at gemma.sudhanva.me with Bearer token authentication.
---

This cluster runs **Gemma 3 1B**, a lightweight text-only LLM from Google, using [Ollama](https://ollama.com) for CPU inference on ARM64 nodes.

## Endpoint

```text
https://gemma.k8s.sudhanva.me
```

:::tip
The API is OpenAI-compatible. Use `/v1/chat/completions` for chat and `/api/generate` for raw generation.
:::

## Authentication

All requests require a Bearer token:

```bash
curl https://gemma.k8s.sudhanva.me/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3:1b",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

## Streaming

For longer generations, it is highly recommended to use **streaming**. This prevents "Gateway Timeout" errors from the OCI Load Balancer or Cloudflare by sending tokens as they are generated.

### Using curl

Add `"stream": true` and use the `-N` (no-buffer) flag:

```bash
curl -N https://gemma.k8s.sudhanva.me/v1/chat/completions \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemma3:1b",
    "messages": [{"role": "user", "content": "Tell me about Bengaluru"}],
    "stream": true
  }'
```

### Using Python

```python
import openai

client = openai.OpenAI(
    base_url="https://gemma.k8s.sudhanva.me/v1",
    api_key="YOUR_API_KEY"
)

response = client.chat.completions.create(
    model="gemma3:1b",
    messages=[{"role": "user", "content": "Tell me about Bengaluru"}],
    stream=True
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

The API key is stored in OCI Vault (`gemma-api-key`) and synced to the cluster via ExternalSecret.

## Architecture

```mermaid
flowchart LR
    User((User)) --> Gateway[Envoy Gateway]
    Gateway --> Auth[OpenResty\nAuth Proxy]
    Auth --> Ollama[Ollama\nGemma 3 1B]

    subgraph Pod["gemma pod"]
        Auth
        Ollama
    end

    Vault[(OCI Vault)] --> ES[ExternalSecret]
    ES --> Secret[K8s Secret]
    Secret --> Auth
```

| Component | Purpose |
|-----------|---------|
| OpenResty sidecar | Validates `Authorization: Bearer` header |
| Ollama container | Runs Gemma 3 1B inference |
| OCI Vault | Stores API key securely |
| ExternalSecret | Syncs API key to cluster |

## Resource Usage

| Resource | Allocated |
|----------|-----------|
| Memory | 2-4 GB |
| CPU | 1-2 cores |
| Model | ~700 MB (Q4 quantized) |

:::caution
First request after pod startup may be slow while Ollama loads the model.

**Important**: If generation takes too long (e.g., complex queries), the connection might timeout before the model finishes. Use **streaming** to avoid this.
:::

## Configuration

The API key is configured in `terraform.tfvars`:

```hcl
gemma_api_key = "your-secret-key"
```

After setting, run `terraform apply` to create the vault secret, then sync `managed-secrets` in ArgoCD.

## Pulling the Model

The model is pulled on first request. To pre-pull:

```bash
kubectl exec -it deploy/gemma -c ollama -- ollama pull gemma3:1b
```

## Available Models

| Model | Size | Use Case |
|-------|------|----------|
| `gemma3:1b` | ~700 MB | Fast, general text |
| `gemma3:1b-it-qat` | ~500 MB | QAT optimized |
